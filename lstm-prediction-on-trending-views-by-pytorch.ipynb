{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Motivation\nIn [my last notebook](https://www.kaggle.com/mengxinbj/lstm-prediction-on-trending-youtube-videos-views), I have built a LSTM model with Keras and predicted views trending of videos of [Trending Youtube Video Statistic dataset](https://www.kaggle.com/datasnaek/youtube-new). In this notobook, I'm going to implement the same prediction task by Pytorch. It's a good example to experience the style of Pytorch. There are more coding in building and training model than in Keras, but it is much more flexible and powerful. I hope you like it. Let's get started."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport os\nfrom sklearn.preprocessing import StandardScaler","execution_count":51,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Read and Proprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath1=\"/kaggle/input/youtube-new/US_category_id.json\"\ncategory_id_df = pd.read_json(filepath1)\ncategory_id_df.head()","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"                                kind  \\\n0  youtube#videoCategoryListResponse   \n1  youtube#videoCategoryListResponse   \n2  youtube#videoCategoryListResponse   \n3  youtube#videoCategoryListResponse   \n4  youtube#videoCategoryListResponse   \n\n                                                etag  \\\n0  \"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...   \n1  \"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...   \n2  \"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...   \n3  \"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...   \n4  \"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...   \n\n                                               items  \n0  {'kind': 'youtube#videoCategory', 'etag': '\"m2...  \n1  {'kind': 'youtube#videoCategory', 'etag': '\"m2...  \n2  {'kind': 'youtube#videoCategory', 'etag': '\"m2...  \n3  {'kind': 'youtube#videoCategory', 'etag': '\"m2...  \n4  {'kind': 'youtube#videoCategory', 'etag': '\"m2...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>kind</th>\n      <th>etag</th>\n      <th>items</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>youtube#videoCategoryListResponse</td>\n      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...</td>\n      <td>{'kind': 'youtube#videoCategory', 'etag': '\"m2...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>youtube#videoCategoryListResponse</td>\n      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...</td>\n      <td>{'kind': 'youtube#videoCategory', 'etag': '\"m2...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>youtube#videoCategoryListResponse</td>\n      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...</td>\n      <td>{'kind': 'youtube#videoCategory', 'etag': '\"m2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>youtube#videoCategoryListResponse</td>\n      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...</td>\n      <td>{'kind': 'youtube#videoCategory', 'etag': '\"m2...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>youtube#videoCategoryListResponse</td>\n      <td>\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJv...</td>\n      <td>{'kind': 'youtube#videoCategory', 'etag': '\"m2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"filepath2=\"/kaggle/input/youtube-new/USvideos.csv\"\nvideos_df = pd.read_csv(filepath2,header='infer')\nvideos_df.head()","execution_count":53,"outputs":[{"output_type":"execute_result","execution_count":53,"data":{"text/plain":"      video_id trending_date  \\\n0  2kyS6SvSYSE      17.14.11   \n1  1ZAPwfrtAFY      17.14.11   \n2  5qpjK5DgCt4      17.14.11   \n3  puqaWrEC7tY      17.14.11   \n4  d380meD0W0M      17.14.11   \n\n                                               title          channel_title  \\\n0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n4                           I Dare You: GOING BALD!?               nigahiga   \n\n   category_id              publish_time  \\\n0           22  2017-11-13T17:13:01.000Z   \n1           24  2017-11-13T07:30:00.000Z   \n2           23  2017-11-12T19:05:24.000Z   \n3           24  2017-11-13T11:00:04.000Z   \n4           24  2017-11-12T18:01:41.000Z   \n\n                                                tags    views   likes  \\\n0                                    SHANtell martin   748374   57527   \n1  last week tonight trump presidency|\"last week ...  2418783   97185   \n2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168   10172   \n4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  132235   \n\n   dislikes  comment_count                                  thumbnail_link  \\\n0      2966          15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n1      6146          12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n2      5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n3       666           2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n4      1989          17518  https://i.ytimg.com/vi/d380meD0W0M/default.jpg   \n\n   comments_disabled  ratings_disabled  video_error_or_removed  \\\n0              False             False                   False   \n1              False             False                   False   \n2              False             False                   False   \n3              False             False                   False   \n4              False             False                   False   \n\n                                         description  \n0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n1  One year after the presidential election, John...  \n2  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  \n3  Today we find out if Link is a Nickelback amat...  \n4  I know it's been a while since we did this sho...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>trending_date</th>\n      <th>title</th>\n      <th>channel_title</th>\n      <th>category_id</th>\n      <th>publish_time</th>\n      <th>tags</th>\n      <th>views</th>\n      <th>likes</th>\n      <th>dislikes</th>\n      <th>comment_count</th>\n      <th>thumbnail_link</th>\n      <th>comments_disabled</th>\n      <th>ratings_disabled</th>\n      <th>video_error_or_removed</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2kyS6SvSYSE</td>\n      <td>17.14.11</td>\n      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n      <td>CaseyNeistat</td>\n      <td>22</td>\n      <td>2017-11-13T17:13:01.000Z</td>\n      <td>SHANtell martin</td>\n      <td>748374</td>\n      <td>57527</td>\n      <td>2966</td>\n      <td>15954</td>\n      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1ZAPwfrtAFY</td>\n      <td>17.14.11</td>\n      <td>The Trump Presidency: Last Week Tonight with J...</td>\n      <td>LastWeekTonight</td>\n      <td>24</td>\n      <td>2017-11-13T07:30:00.000Z</td>\n      <td>last week tonight trump presidency|\"last week ...</td>\n      <td>2418783</td>\n      <td>97185</td>\n      <td>6146</td>\n      <td>12703</td>\n      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>One year after the presidential election, John...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5qpjK5DgCt4</td>\n      <td>17.14.11</td>\n      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n      <td>Rudy Mancuso</td>\n      <td>23</td>\n      <td>2017-11-12T19:05:24.000Z</td>\n      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n      <td>3191434</td>\n      <td>146033</td>\n      <td>5339</td>\n      <td>8181</td>\n      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>puqaWrEC7tY</td>\n      <td>17.14.11</td>\n      <td>Nickelback Lyrics: Real or Fake?</td>\n      <td>Good Mythical Morning</td>\n      <td>24</td>\n      <td>2017-11-13T11:00:04.000Z</td>\n      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n      <td>343168</td>\n      <td>10172</td>\n      <td>666</td>\n      <td>2146</td>\n      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>Today we find out if Link is a Nickelback amat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d380meD0W0M</td>\n      <td>17.14.11</td>\n      <td>I Dare You: GOING BALD!?</td>\n      <td>nigahiga</td>\n      <td>24</td>\n      <td>2017-11-12T18:01:41.000Z</td>\n      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n      <td>2095731</td>\n      <td>132235</td>\n      <td>1989</td>\n      <td>17518</td>\n      <td>https://i.ytimg.com/vi/d380meD0W0M/default.jpg</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>I know it's been a while since we did this sho...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"In the section, I found and removed some  unnecessary chars like '\"' ',' '\\r' in title,description and channel title columns.These chars may cause problems when trying to load data into relational database for ETL task(We don't do that in this project). Each country may have different chars need to be removed. We only take US dataset as an example."},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_video_csv(video_df,country_code):\n    \"\"\"\n    This function is to remove unnecessary chars like '\"',',','\\r'which will cause errors when copy csv files into Redshift staging table.\n    \n    Parameters:\n    video_df: Dataframe from read_csv file\n    filepath: videos csv filepath\n    \n    Return:\n    video_df: Dataframe which remove unnecessary chars\n    \"\"\"\n    video_df[\"tags\"] = video_df[\"tags\"].apply(lambda x:x.replace('\"',\"\"))\n    video_df[\"title\"] = video_df[\"title\"].apply(lambda x:x.replace(',',' '))\n    video_df[\"channel_title\"] = video_df[\"channel_title\"].apply(lambda x:x.replace(',',' '))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\\r',''))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace(',',' '))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\"',''))\n    video_df[\"country\"] = country_code\n    return video_df\n#Clean videos csv files for selected country code\ncountry_code=['US']\nfor c in country_code:\n    filepath=\"/kaggle/input/youtube-new/\"+c+\"videos.csv\"\n    video_df = pd.read_csv(filepath,header='infer')\n    savepath = \"/kaggle/working/\"+c+\"videos1.csv\"\n    video_df = clean_video_csv(video_df,c)\n    video_df.to_csv(savepath,index=False)","execution_count":54,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since category id in different countries are not the same, we extract categoty id and titles from json file of each country and save it as a csv file for later use. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def category_extract (df,country_code):\n    \"\"\"\n    The function is to extract category id and category title from category_id json files\n    \n    Parameters:\n    df: Dataframe of read_json file\n    filepath: category_id json filepath\n    \n    Return:\n    category_df: Dataframe with columns: category_id,category_title,category_filename,country_code\n    \n    \"\"\"\n    category_id = []\n    category_title = []\n    for i in range(df.shape[0]):\n        category_id.append(df.iloc[i][\"items\"]['id'])\n        category_title.append(df.iloc[i][\"items\"][\"snippet\"][\"title\"])\n    category_df = pd.DataFrame()\n    category_df[\"category_id\"] = category_id\n    category_df[\"category_title\"] = category_title\n    category_df.insert(category_df.shape[1],\"country_code\",country_code)\n    return category_df\n\n#Extract category title and id from json file of each country\ncategory_all = pd.DataFrame()\nfor c in country_code:\n    filepath=\"/kaggle/input/youtube-new/\"+c+\"_category_id.json\"\n    category_id_df = pd.read_json(filepath)\n    category_all = pd.concat([category_all,category_extract(category_id_df,c)])\n    \n#category_all.tail()\nsavepath = \"/kaggle/working/category_all.csv\"\ncategory_all.to_csv(savepath,index=False)","execution_count":55,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Transformation\nMerge the category into trending videos data."},{"metadata":{"trusted":true},"cell_type":"code","source":"US = pd.read_csv(\"/kaggle/working/USvideos1.csv\")\ncategory = pd.read_csv(\"/kaggle/working/category_all.csv\")\nUS1 =US.merge(category,how=\"inner\",left_on=[\"category_id\",\"country\"],right_on=[\"category_id\",\"country_code\"])\nUS1[\"trending_date1\"] = US1[\"trending_date\"].apply(lambda x: pd.Timestamp(int(\"20\"+x[0:2]),int(x[-2:]),int(x[3:5]),0))","execution_count":56,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We extract the columns would be used in this project."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [\"video_id\",\"trending_date1\",\"channel_title\",\"publish_time\",\"views\",\"likes\",\"dislikes\",\"comment_count\",\"category_title\"]\nUS1 = US1[columns].copy()","execution_count":57,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We only keep videos that have more than 4 trending days data. We are going to use the previous 4 trending days data to predict the 5th trending day data."},{"metadata":{"trusted":true},"cell_type":"code","source":"trendingdate_df = US1.groupby(\"video_id\").trending_date1.describe(datetime_is_numeric=True).reset_index()\nvideos = trendingdate_df[trendingdate_df[\"count\"].values>4].video_id\nUS2 = US1[ US1.video_id.isin(videos.values)]\ntrendingdate_df.head()","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"      video_id count                mean        min                 25%  \\\n0  -0CMnp02rNY     6 2018-06-08 12:00:00 2018-06-06 2018-06-07 06:00:00   \n1  -0NYY8cqdiQ     1 2018-02-01 00:00:00 2018-02-01 2018-02-01 00:00:00   \n2  -1Hm41N0dUs     3 2018-04-30 00:00:00 2018-04-29 2018-04-29 12:00:00   \n3  -1yT-K3c6YI     4 2017-11-30 12:00:00 2017-11-29 2017-11-29 18:00:00   \n4  -2RVw2_QyxQ     3 2017-11-15 00:00:00 2017-11-14 2017-11-14 12:00:00   \n\n                  50%                 75%        max  \n0 2018-06-08 12:00:00 2018-06-09 18:00:00 2018-06-11  \n1 2018-02-01 00:00:00 2018-02-01 00:00:00 2018-02-01  \n2 2018-04-30 00:00:00 2018-04-30 12:00:00 2018-05-01  \n3 2017-11-30 12:00:00 2017-12-01 06:00:00 2017-12-02  \n4 2017-11-15 00:00:00 2017-11-15 12:00:00 2017-11-16  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>count</th>\n      <th>mean</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0CMnp02rNY</td>\n      <td>6</td>\n      <td>2018-06-08 12:00:00</td>\n      <td>2018-06-06</td>\n      <td>2018-06-07 06:00:00</td>\n      <td>2018-06-08 12:00:00</td>\n      <td>2018-06-09 18:00:00</td>\n      <td>2018-06-11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0NYY8cqdiQ</td>\n      <td>1</td>\n      <td>2018-02-01 00:00:00</td>\n      <td>2018-02-01</td>\n      <td>2018-02-01 00:00:00</td>\n      <td>2018-02-01 00:00:00</td>\n      <td>2018-02-01 00:00:00</td>\n      <td>2018-02-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1Hm41N0dUs</td>\n      <td>3</td>\n      <td>2018-04-30 00:00:00</td>\n      <td>2018-04-29</td>\n      <td>2018-04-29 12:00:00</td>\n      <td>2018-04-30 00:00:00</td>\n      <td>2018-04-30 12:00:00</td>\n      <td>2018-05-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1yT-K3c6YI</td>\n      <td>4</td>\n      <td>2017-11-30 12:00:00</td>\n      <td>2017-11-29</td>\n      <td>2017-11-29 18:00:00</td>\n      <td>2017-11-30 12:00:00</td>\n      <td>2017-12-01 06:00:00</td>\n      <td>2017-12-02</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-2RVw2_QyxQ</td>\n      <td>3</td>\n      <td>2017-11-15 00:00:00</td>\n      <td>2017-11-14</td>\n      <td>2017-11-14 12:00:00</td>\n      <td>2017-11-15 00:00:00</td>\n      <td>2017-11-15 12:00:00</td>\n      <td>2017-11-16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We standardize views data before feed it into model later."},{"metadata":{"trusted":true},"cell_type":"code","source":"def standardize(data):\n    scaler = StandardScaler()\n    scaler = scaler.fit(data)\n    transformed = scaler.transform(data)\n    return scaler,transformed\nscaler_views, US_views = standardize(US2.views.values.reshape(-1,1))\n#scaler_likes, US_likes = standardize(US2.likes.values.reshape(-1,1))\n#scaler_dislikes, US_dislikes = standardize(US2.dislikes.values.reshape(-1,1))\n#scaler_comments, US_comments = standardize(US2.comment_count.values.reshape(-1,1))","execution_count":59,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We build the data subset would be used in building and trainging model."},{"metadata":{"trusted":true},"cell_type":"code","source":"US3 = pd.DataFrame()\nUS3[\"trending_date1\"] = US2[\"trending_date1\"]\nUS3[\"video_id\"] = US2[\"video_id\"]\nUS3[\"views\"] = US_views\n#US3[\"likes\"] = US_likes\n#US3[\"dislikes\"] = US_dislikes\n#US3[\"comment_count\"] = US_comments\nUS3.reset_index(inplace=True)\nUS3.head()","execution_count":60,"outputs":[{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"   index trending_date1     video_id     views\n0      0     2017-11-14  2kyS6SvSYSE -0.237504\n1     17     2017-11-15  2kyS6SvSYSE -0.055430\n2     31     2017-11-16  cmoknv58jjE -0.104086\n3     32     2017-11-16  2kyS6SvSYSE -0.038156\n4     33     2017-11-16  Jidk0O6uu-0 -0.325464","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>trending_date1</th>\n      <th>video_id</th>\n      <th>views</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2017-11-14</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.237504</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17</td>\n      <td>2017-11-15</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.055430</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31</td>\n      <td>2017-11-16</td>\n      <td>cmoknv58jjE</td>\n      <td>-0.104086</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>32</td>\n      <td>2017-11-16</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.038156</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n      <td>2017-11-16</td>\n      <td>Jidk0O6uu-0</td>\n      <td>-0.325464</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"US3.drop(\"index\",axis=1,inplace=True)\nUS3.head()","execution_count":61,"outputs":[{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"  trending_date1     video_id     views\n0     2017-11-14  2kyS6SvSYSE -0.237504\n1     2017-11-15  2kyS6SvSYSE -0.055430\n2     2017-11-16  cmoknv58jjE -0.104086\n3     2017-11-16  2kyS6SvSYSE -0.038156\n4     2017-11-16  Jidk0O6uu-0 -0.325464","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>trending_date1</th>\n      <th>video_id</th>\n      <th>views</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-11-14</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.237504</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-11-15</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.055430</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-11-16</td>\n      <td>cmoknv58jjE</td>\n      <td>-0.104086</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-11-16</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.038156</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-11-16</td>\n      <td>Jidk0O6uu-0</td>\n      <td>-0.325464</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"We build x dataset(features) and y dataset(labels).X contained the previous 4 trending days views and Y is the fifth trending day views."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nx=[]\ny=[]\ncategory = []\nfor v in videos:\n    row=[]\n    temp_df = US3[US3[\"video_id\"]==v].sort_values(by=\"trending_date1\")\n    #print (temp_df)\n    seq = temp_df.views[0:4].index\n        \n    for s in seq:\n        #print (US3.iloc[s].values[2:])\n        row.append(US3.iloc[s].values[2:3])\n    x.append(row)\n    nextstep = temp_df.views[4:5].values\n    y.append(nextstep)\n    ","execution_count":62,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shaping X(total,seqlen,inputdim) and Y(total,outputdim).seqlen=4,inputdim=1, outputdim=1"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.reshape(x,(len(x),4,1))\nprint (x.shape)\nprint (x[0])\n","execution_count":63,"outputs":[{"output_type":"stream","text":"(3984, 4, 1)\n[[-0.2719427408449598]\n [-0.2555659799132111]\n [-0.24286315271238587]\n [-0.23675864020197335]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.reshape(y,(-1,1))\nprint (y.shape)\nprint (y[0])","execution_count":64,"outputs":[{"output_type":"stream","text":"(3984, 1)\n[-0.23191164]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x.astype('float32')\ny = y.astype('float32')","execution_count":65,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train,validation and test data split. Each one should be integral multiples of batchsize."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbatch_size = 100\n\nx_train,x_remain = x[:3700],x[3700:3900]\ny_train,y_remain = y[:3700],y[3700:3900]\n\nx_val,x_test = x_remain[:100],x_remain[100:]\ny_val,y_test = y_remain[:100],y_remain[100:]\nprint (x_train.shape,x_val.shape,x_test.shape)","execution_count":66,"outputs":[{"output_type":"stream","text":"(3700, 4, 1) (100, 4, 1) (100, 4, 1)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Create tensor datasets. Create DataLoaders and batch the training, validation and test dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset,DataLoader\ntrain_data = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_val),torch.from_numpy(y_val))\ntest_data = TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n\n\ntrain_loader = DataLoader(train_data,shuffle=True,batch_size=batch_size)\nvalid_loader = DataLoader(valid_data,shuffle=True,batch_size=batch_size)\ntest_loader = DataLoader(test_data,shuffle=True,batch_size=batch_size)","execution_count":67,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have a look at the X and Y dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataiter = iter(train_loader)\nsample_x,sample_y = dataiter.next()\n\nprint ('Sample input size:',sample_x.size())\nprint ('Sample input:\\n',sample_x[0:2])\nprint ('\\n')\nprint ('Sample output size:',sample_y.size())\nprint ('Sample output:\\n',sample_y[0:2])\n","execution_count":68,"outputs":[{"output_type":"stream","text":"Sample input size: torch.Size([100, 4, 1])\nSample input:\n tensor([[[-0.0899],\n         [ 1.3740],\n         [ 1.5428],\n         [ 1.7263]],\n\n        [[-0.2691],\n         [-0.2090],\n         [-0.1870],\n         [-0.1702]]])\n\n\nSample output size: torch.Size([100, 1])\nSample output:\n tensor([[ 1.9194],\n        [-0.1596]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Build the Model"},{"metadata":{},"cell_type":"markdown","source":"Checking if GPU is available."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\nif train_on_gpu:\n    print ('Training on GPU')\nelse:\n    print ('Training on CPU')","execution_count":69,"outputs":[{"output_type":"stream","text":"Training on CPU\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Build model with multilayer LSTM ,dropout layer and a full connected layer at the end. The model takes input_size,output_size,hidden_dim,number of layers(LSTM) and dropout prob(between LSTM layers) and set batch_first is True."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nclass mylstm(nn.Module):\n    \n    def __init__(self,input_size,output_size,hidden_dim, n_layers,drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(mylstm,self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.lstm = nn.LSTM(input_size,hidden_dim,n_layers,batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim,output_size)\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        \n        lstm_out,hidden = self.lstm(x,hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out =self.dropout(lstm_out)\n       \n        out = self.fc(out)#shape=(batchsize,seqlen,outputdim)\n        out = out.view(batch_size, -1) \n        out = out[:, -1]\n        return out,hidden\n    \n    def init_hidden(self,batch_size):\n        \"\"\"\n        Initializes hidden state and cell state\n        \"\"\"\n        \n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = 1\noutput_size =1\nhidden_dim = 256\nn_layers = 2\n\nnet = mylstm(input_size,output_size,hidden_dim,n_layers)\nprint (net)","execution_count":71,"outputs":[{"output_type":"stream","text":"mylstm(\n  (lstm): LSTM(1, 256, num_layers=2, batch_first=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Training\nWe set criterion as MSEloss and optimizer as Adam.We have 40 loops for the whole trainset. Evaluate on validationset and print result every 50 training samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = 0.001\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(),lr=lr)","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epoch = 40\ncounter = 0\nprint_every = 50\nclip = 5\n\nif train_on_gpu:\n    net.cuda()\n    \nnet.train()\nfor e in range(epoch):\n    h = net.init_hidden(batch_size)\n    #print (h[0].shape)\n    for inputs,labels in train_loader:\n        counter +=1\n        \n        if train_on_gpu:\n            inputs,labels = inputs.cuda(),output.cuda()\n            \n        h = tuple([each.data for each in h])\n        net.zero_grad()\n        output,h = net(inputs,h)\n        \n        loss = criterion(output.squeeze(),labels.squeeze())\n        loss.backward()\n        \n        nn.utils.clip_grad_norm_(net.parameters(),clip)\n        optimizer.step()\n        \n        if counter % print_every==0:\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs,labels in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n            \n                if train_on_gpu:\n                    inputs,labels = inputs.cuda(),labels.cuda()\n                output,val_h = net(inputs,val_h)\n                val_loss = criterion(output.squeeze(),labels.squeeze())\n                \n                val_losses.append(val_loss.item())\n            net.train()\n            print (\"Epoch:{}/{}...\".format(e+1,epoch),\n               \"Step:{}...\".format(counter),\n               \"Loss:{:.6f}...\".format(loss.item()),\n               \"Val loss:{:.6f}\".format(np.mean(val_losses)))","execution_count":73,"outputs":[{"output_type":"stream","text":"Epoch:2/40... Step:50... Loss:0.012888... Val loss:0.008803\nEpoch:3/40... Step:100... Loss:0.011558... Val loss:0.004719\nEpoch:5/40... Step:150... Loss:0.189438... Val loss:0.004063\nEpoch:6/40... Step:200... Loss:0.068017... Val loss:0.022945\nEpoch:7/40... Step:250... Loss:0.000795... Val loss:0.002629\nEpoch:9/40... Step:300... Loss:0.004782... Val loss:0.006457\nEpoch:10/40... Step:350... Loss:0.031094... Val loss:0.001639\nEpoch:11/40... Step:400... Loss:0.002246... Val loss:0.002133\nEpoch:13/40... Step:450... Loss:0.026684... Val loss:0.002824\nEpoch:14/40... Step:500... Loss:0.003164... Val loss:0.002069\nEpoch:15/40... Step:550... Loss:0.003797... Val loss:0.002123\nEpoch:17/40... Step:600... Loss:0.002095... Val loss:0.002428\nEpoch:18/40... Step:650... Loss:0.003948... Val loss:0.001548\nEpoch:19/40... Step:700... Loss:0.002160... Val loss:0.001442\nEpoch:21/40... Step:750... Loss:0.000857... Val loss:0.001377\nEpoch:22/40... Step:800... Loss:0.001483... Val loss:0.002395\nEpoch:23/40... Step:850... Loss:0.001051... Val loss:0.001017\nEpoch:25/40... Step:900... Loss:0.000858... Val loss:0.000619\nEpoch:26/40... Step:950... Loss:0.002261... Val loss:0.008289\nEpoch:28/40... Step:1000... Loss:0.001201... Val loss:0.000760\nEpoch:29/40... Step:1050... Loss:0.000591... Val loss:0.000868\nEpoch:30/40... Step:1100... Loss:0.003236... Val loss:0.011277\nEpoch:32/40... Step:1150... Loss:0.013562... Val loss:0.000344\nEpoch:33/40... Step:1200... Loss:0.005375... Val loss:0.001778\nEpoch:34/40... Step:1250... Loss:0.009429... Val loss:0.000293\nEpoch:36/40... Step:1300... Loss:0.000293... Val loss:0.000850\nEpoch:37/40... Step:1350... Loss:0.001412... Val loss:0.001789\nEpoch:38/40... Step:1400... Loss:0.001387... Val loss:0.001693\nEpoch:40/40... Step:1450... Loss:0.002804... Val loss:0.002822\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We test on test set,which only have 1 batch.We print the loss. Be notice that here the data is still standardized. They haven't been inversely transformed."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_losses=[]\n\nh = net.init_hidden(batch_size)\nnet.eval()\nfor inputs,labels in test_loader:\n    h = tuple([each.data for each in h])\n    if train_on_gpu:\n        inputs,labels =inputs.cuda(),labels.cuda()\n    output,h = net(inputs,h)\n    test_loss = criterion(output.squeeze(),labels.squeeze())\n    \n    test_losses.append(test_loss.item())\n\nprint (\"Test loss:{:.6f}\".format(np.mean(test_losses)))\n\n\n\n","execution_count":74,"outputs":[{"output_type":"stream","text":"Test loss:0.001955\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We predict on a slice of dataset which never been seen by train,validation and test set. And we print the demo true and prediction values. They are quite close. Finally we print the root mean square error of the small prediction demo dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import Tensor \nfrom sklearn.metrics import mean_squared_error\nimport math\npredict_tensor = torch.from_numpy(x[3900:3910])\nbatch_size = predict_tensor.size(0)\nh = net.init_hidden(batch_size)\nif train_on_gpu:\n    predict_tensor = predict_tensor.cuda()\noutput,h = net(predict_tensor,h)\npred = scaler_views.inverse_transform(Tensor.detach(output) )\ntrue = scaler_views.inverse_transform(y[3900:3910])\nfor i in range(10):\n    print (\"The true value is {}, and the predict value is {}\".format(true[i],pred[i]))\nprint (\"The root mean square error of the 10 data is \",math.sqrt(mean_squared_error(pred,true)))","execution_count":75,"outputs":[{"output_type":"stream","text":"The true value is [3017144.], and the predict value is 3253285.0\nThe true value is [237230.97], and the predict value is 240166.21875\nThe true value is [460125.97], and the predict value is 472155.71875\nThe true value is [1899969.], and the predict value is 2014338.25\nThe true value is [879709.94], and the predict value is 860050.3125\nThe true value is [1827806.], and the predict value is 1916636.0\nThe true value is [96253.21], and the predict value is 85471.7109375\nThe true value is [351798.97], and the predict value is 359858.21875\nThe true value is [99200.96], and the predict value is 86233.9609375\nThe true value is [608431.06], and the predict value is 609081.6875\nThe root mean square error of the 10 data is  88104.06065556797\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"We see the average percentage error of the slice of 10 dataset is 2.35%. It's pretty good!"},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"The average percentage error of the 10 data is {:.3f}%\".format(np.mean((pred-true)/true)))","execution_count":83,"outputs":[{"output_type":"stream","text":"The average percentage error of the 10 data is 2.354%\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}