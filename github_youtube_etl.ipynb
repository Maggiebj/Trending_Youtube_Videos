{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trending YouTube Videos Data ETL Pipeline \n",
    "\n",
    "\n",
    "#### Project Summary\n",
    "YouTube (the world-famous video sharing website) maintains a List of the top trending videos on the platform. According to Variety magazine, “To determine the year’s top-trending videos, YouTube uses a combination of factors including measuring users interactions (number of views, shares, comments And likes). Note that they’re Not the most-viewed videos overall For the calendar year”.\n",
    "\n",
    "\n",
    "\n",
    "#### Dataset Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project And Gather Data\n",
    "* Step 2: Explore And Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import configparser\n",
    "from sql_queries import create_table_queries, drop_table_queries,check_table_queries \n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import weekofyear,hour,dayofmonth,month,year,dayofweek\n",
    "from pyspark.sql.types import IntegerType ,TimestampType\n",
    "from pyspark.sql.functions import udf, col,from_unixtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope the Project\n",
    "The motivation of the project is to built ETL automation pipeline to extract data from raw csv files and json files, design and built a database schema in AWS Redshift Data Warehouse ,transform and load data in the database tables to facilitate data analysis and know about how videos trends going in different countries.\n",
    "\n",
    "This dataset is a daily record of the top trending YouTube videos. The dataset is public available on https://www.kaggle.com/datasnaek/youtube-new\n",
    "\n",
    "Tools used:\n",
    "1. Data Warehouse: AWS Redshift\n",
    "2. SPARK SQL\n",
    "3. Python 3.X\n",
    "\n",
    "#### Data Description\n",
    "This dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US, GB, DE, CA, and FR regions (USA, Great Britain, Germany, Canada, and France, respectively), with up to 200 listed trending videos per day.\n",
    "\n",
    "EDIT: Now includes data from RU, MX, KR, JP and IN regions (Russia, Mexico, South Korea, Japan and India respectively) over the same time period.\n",
    "\n",
    "Each region’s data is in a separate file. Data includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count.\n",
    "\n",
    "The data also includes a category_id field, which varies between regions. To retrieve the categories for a specific video, find it in the associated JSON. One such file is included for each of the regions in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath1=\"category_id/US_category_id.json\"\n",
    "category_id_df = pd.read_json(filepath1)\n",
    "category_id_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath2=\"country_videos/USvideos.csv\"\n",
    "videos_df = pd.read_csv(filepath2,header='infer')\n",
    "videos_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "When we explore the raw videos csv files, we find some chars like '\"' in tags column,',' in title column and '\\r' in description column, etc. These chars will cause error when copying csv files into Redshift staging table. We will check and remove these chars before copy data into Redshift.\n",
    "\n",
    "When we explore the category_id json file, we find it has nested structure. One json file for one country. We will extract category id and category title of each country from these json files.\n",
    "#### Cleaning Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_video_csv(video_df,country_code):\n",
    "    \"\"\"\n",
    "    This function is to remove unnecessary chars like '\"',',','\\r'which will cause errors when copy csv files into Redshift staging table.\n",
    "    \n",
    "    Parameters:\n",
    "    video_df: Dataframe from read_csv file\n",
    "    filepath: videos csv filepath\n",
    "    \n",
    "    Return:\n",
    "    video_df: Dataframe which remove unnecessary chars\n",
    "    \"\"\"\n",
    "    video_df[\"tags\"] = video_df[\"tags\"].apply(lambda x:x.replace('\"',\"\"))\n",
    "    video_df[\"title\"] = video_df[\"title\"].apply(lambda x:x.replace(',',' '))\n",
    "    video_df[\"channel_title\"] = video_df[\"channel_title\"].apply(lambda x:x.replace(',',' '))\n",
    "    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\\r',''))\n",
    "    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace(',',' '))\n",
    "    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\"',''))\n",
    "    video_df[\"country\"] = country_code\n",
    "    return video_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the country code for ETL. The available country code are US, GB, DE, CA, FR ,RU, MX, KR, JP and IN regions (USA, Great Britain, Germany, Canada, France, Russia, Mexico, South Korea, Japan and India respectively) over the same time period. Be careful that each csv file may has different problematic chars to remove. Each file has to be inspected respectively. Here I only worked throught USvideos.csv file as demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean videos csv files for selected country code\n",
    "country_code=['US']\n",
    "for c in country_code:\n",
    "    filepath=\"country_videos/\"+c+\"videos.csv\"\n",
    "    video_df = pd.read_csv(filepath,header='infer')\n",
    "    savepath = filepath[:filepath.find(\".csv\")]+'1.csv'\n",
    "    video_df = clean_video_csv(video_df,c)\n",
    "    video_df.to_csv(savepath,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_extract (df,country_code):\n",
    "    \"\"\"\n",
    "    The function is to extract category id and category title from category_id json files\n",
    "    \n",
    "    Parameters:\n",
    "    df: Dataframe of read_json file\n",
    "    filepath: category_id json filepath\n",
    "    \n",
    "    Return:\n",
    "    category_df: Dataframe with columns: category_id,category_title,category_filename,country_code\n",
    "    \n",
    "    \"\"\"\n",
    "    category_id = []\n",
    "    category_title = []\n",
    "    for i in range(df.shape[0]):\n",
    "        category_id.append(df.iloc[i][\"items\"]['id'])\n",
    "        category_title.append(df.iloc[i][\"items\"][\"snippet\"][\"title\"])\n",
    "    category_df = pd.DataFrame()\n",
    "    category_df[\"category_id\"] = category_id\n",
    "    category_df[\"category_title\"] = category_title\n",
    "    category_df.insert(category_df.shape[1],\"country_code\",country_code)\n",
    "    return category_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract category title and id from json file of each country\n",
    "category_all = pd.DataFrame()\n",
    "for c in country_code:\n",
    "    filepath=\"category_id/\"+c+\"_category_id.json\"\n",
    "    category_id_df = pd.read_json(filepath)\n",
    "    category_all = pd.concat([category_all,category_extract(category_id_df,c)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category_all.tail()\n",
    "savepath = \"category_id/\"+\"category_all.csv\"\n",
    "category_all.to_csv(savepath,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "After we prepare the staging table of videos, we need to think about the data model in our Redshift database. It means the schema of fact table and dimension tables. In the cleaned videos csv files, we find that one video appears several time on different trending_date. There are redundency data in the staging table(cleaned csv file). We can split the staging table into fact and dimension tables. We also need involve category title from category_id json file which is distinct from each country.\n",
    "\n",
    "**Fact Tables**\n",
    "#### videos_trending\n",
    "*videos_trending_id,video_id,trending_date,views,likes,dislikes,comment_count,comments_disabled,ratings_disabled,video_error_or_removed,country*\n",
    "\n",
    "**Dimension Tables**\n",
    "#### videos \n",
    "*video_id,title,channel_title,category_title,country,publish_time,tags,thumbnail_link,description*\n",
    "\n",
    "#### videos _notitle\n",
    "*video_id,channel_title,category_title,country,publish_time*\n",
    "\n",
    "#### time\n",
    "*trending_date,year,month,day*\n",
    "\n",
    "#### category\n",
    "*category_id,category_title,country_code*\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Here is the steps to pipeline the data into the data model\n",
    "* 1. Create the data model\n",
    "* 2. COPY cleaned videos csv into staging table\n",
    "* 3. COPY extracted category csv into category table\n",
    "* 4. Transform and load data into videos_trending, videos,time tables\n",
    "* 5. Data quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables(cur,conn):\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(cur,conn):\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_tables(cur,conn)\n",
    "create_tables(cur,conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 COPY cleaned videos csv into staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID']=config['CREDENTIAL']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY']\n",
    "output_datapath_s3a = \"s3a://XXX/\"\n",
    "output_datapath_s3 = \"s3://XXX/\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] ='/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.5\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_pd = pd.DataFrame()\n",
    "country_code = ['US']\n",
    "for c in country_code:\n",
    "    filepath=\"country_videos/\"+c+\"videos1.csv\"\n",
    "    df= spark.read.csv(filepath,inferSchema=True,header=True,sep=',')\n",
    "    videos_pd =pd.concat([videos_pd,df.toPandas()],axis=0)\n",
    "df_videos = spark.createDataFrame(videos_pd)\n",
    "df_videos.write.csv(output_datapath_s3a+\"videos_all\",mode='append',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_staging_sql = \"\"\"\n",
    "copy {} \n",
    "from '{}'\n",
    "ACCESS_KEY_ID '{}'\n",
    "SECRET_ACCESS_KEY '{}'\n",
    "delimiter ','\n",
    "ignoreheader 1\n",
    "timeformat as 'YYYY-MM-DDTHH24:MI:SS.000Z'\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_sql = copy_staging_sql.format(\"youtube.staging_videos\",output_datapath_s3+\"videos_all/\",config['CREDENTIAL']['AWS_ACCESS_KEY_ID'],config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY'])\n",
    "cur.execute(copy_sql)\n",
    "conn.commit()\n",
    "print (\"Copying videos csv to staging_videos table is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 COPY extracted category csv into category table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"category_id/category_all.csv\"\n",
    "df= spark.read.csv(filepath,inferSchema=True,header=True,sep=',')\n",
    "df.write.csv(output_datapath_s3a+\"category_all\",header=True)\n",
    "df.createOrReplaceTempView(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_sql = copy_staging_sql.format(\"youtube.category\",output_datapath_s3+\"category_all/\",config['CREDENTIAL']['AWS_ACCESS_KEY_ID'],config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY'])\n",
    "cur.execute(copy_sql)\n",
    "conn.commit()\n",
    "print (\"Copying category to category table is completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Transform and load data into videos_trending, videos,time tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.1 Transform and load data into videos_trending table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_videos.createOrReplaceTempView(\"staging_videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_trending = spark.sql('''\n",
    "SELECT DISTINCT video_id,trending_date,views,likes,dislikes,comment_count,comments_disabled,ratings_disabled,video_error_or_removed,country\n",
    "FROM staging_videos s\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_trending.write.csv(output_datapath_s3a+\"videos_trending\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_sql = copy_staging_sql.format(\"youtube.videos_trending\",output_datapath_s3+\"videos_trending/\",config['CREDENTIAL']['AWS_ACCESS_KEY_ID'],config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY'])\n",
    "cur.execute(copy_sql)\n",
    "conn.commit()\n",
    "print (\"Copying videos_trending to videos_trending table is completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.2 Transform and load data into video table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = spark.sql('''\n",
    "SELECT DISTINCT video_id,title,channel_title,category_title,s.country,publish_time,tags,thumbnail_link,description\n",
    "FROM staging_videos s\n",
    "JOIN category c\n",
    "ON s.category_id = c.category_id AND s.country =c.country_code\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.write.csv(output_datapath_s3a+\"videos\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_sql = copy_staging_sql.format(\"youtube.videos\",output_datapath_s3+\"videos/\",config['CREDENTIAL']['AWS_ACCESS_KEY_ID'],config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY'])\n",
    "cur.execute(copy_sql)\n",
    "conn.commit()\n",
    "print (\"Copying videos to videos table is completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that several video titles ,tags and descriptions may related to one video_id. We load videos without titles ,tags and discriptions in table youtube.videos_notitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_notitle = spark.sql('''\n",
    "SELECT DISTINCT video_id,channel_title,category_title,s.country,publish_time\n",
    "FROM staging_videos s\n",
    "JOIN category c\n",
    "ON s.category_id = c.category_id AND s.country =c.country_code\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_notitle.write.csv(output_datapath_s3a+\"videos_notitle\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_sql = copy_staging_sql.format(\"youtube.videos_notitle\",output_datapath_s3+\"videos_notitle/\",config['CREDENTIAL']['AWS_ACCESS_KEY_ID'],config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY'])\n",
    "cur.execute(copy_sql)\n",
    "conn.commit()\n",
    "print (\"Copying videos to videos table is completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4.3 Transform and load data into time table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = spark.sql('''\n",
    "SELECT DISTINCT trending_date\n",
    "FROM staging_videos\n",
    "''')\n",
    "time_pandas = time.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pandas[\"year\"] = time_pandas[\"trending_date\"].apply(lambda x: \"20\"+x[0:2])\n",
    "time_pandas[\"month\"] = time_pandas[\"trending_date\"].apply(lambda x: x[-2:])\n",
    "time_pandas[\"day\"] = time_pandas[\"trending_date\"].apply(lambda x: x[3:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = spark.createDataFrame(time_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df.write.csv(output_datapath_s3a+\"time\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_sql = copy_staging_sql.format(\"youtube.time\",output_datapath_s3+\"time\",config['CREDENTIAL']['AWS_ACCESS_KEY_ID'],config['CREDENTIAL']['AWS_SECRET_ACCESS_KEY'])\n",
    "cur.execute(copy_sql)\n",
    "conn.commit()\n",
    "print (\"Copying time data to time table is completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Each table has its primary key. Redshift does not enforced primary key to be unique, we ensure it by data processing. The foreign key is \"video_id\" which is set to DISTKEY in fact table and dimension tables to eliminate suffleing and fast table joins. We also set time and category table to DISTSTYLE ALL, for the two tables are small and be frequently used.\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_check(table):\n",
    "    table_check_sql1 = \"\"\"\n",
    "    SELECT * \n",
    "    FROM {}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    table_check_sql2 = \"\"\"\n",
    "    SELECT COUNT(*)\n",
    "    FROM {}\n",
    "    \"\"\"\n",
    "    check_sql = table_check_sql1.format(table)\n",
    "    cur.execute(check_sql)\n",
    "    conn.commit()\n",
    "    rows = cur.fetchall()\n",
    "    print (\"{} table check:\\n\".format(table))\n",
    "    for r in rows:\n",
    "        print (r)\n",
    "    check_sql = table_check_sql2.format(table)\n",
    "    cur.execute(check_sql)\n",
    "    conn.commit()\n",
    "    rowcount = cur.fetchall()\n",
    "    print (\"\\n{} table has {} rows\\n\".format(table,rowcount[0][0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in check_table_queries:\n",
    "    \n",
    "    table_check(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "##### 1. The rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "* In this project, I choose Redshift as Data Warehouse and spark to do ETL. Redshift is a Data Warehouse of AWS which is easy obtained with low costs. \n",
    "\n",
    "* I choose Spark to do ETL other than Redshift SQL because Spark is a powerful tool to directly infer file schema from datalake, which is an edge for loading big data in relational database and it is much faster than INSERT command of SQL. By Spark, I built data files in datalake meanwhile built Database in Redshift.\n",
    "\n",
    "* Before moving to copy csv files to Redshift staging tables , I found there are many chars in original csv files which cause errors when copying. These chars include '\"', ',', '\\r',etc. I found pandas is a powerful tool to handle the problem. Pandas can correctly build DataFrame without the influence of these chars and make it easy to remove them. Original csv files of each country have different problem may need to be inspected respectively. I only worked through the USvideos.csv as demo. The cleaning method is general.\n",
    "\n",
    "##### 2. The data should be update daily according to \"trending_date\"\n",
    "\n",
    "##### 3. Discussion:\n",
    "* 1. If the data was increased by 100x, it is may outrange the ability of pandas. I can use Spark to leverage its distributed computing ability to clean original csv files and do ETL tasks as well.\n",
    "* 2. If the data populates a dashboard that must be updated on a daily basis by 7am everyday, it is a proper way to do daily ETL by Airflow and schedule email reports of any tasks errors.\n",
    "* 3. Redshift Database can be accessed by 100+ people at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
